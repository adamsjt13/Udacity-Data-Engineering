{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data Pipeline\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Perform ETL on multiple datasets containing immigration, airport, and U.S. state demographics. Result will be clean analytics table containing only data for trips to the 50 U.S. states. The final tables should be initiutive and easy to use by anyone interested in analyzing immigration logistic and demographic information.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Scope the Project and Gather Data\n",
    "\n",
    "This project will take US immigration related-data from various sources and create clean tables to aid in analytics and visualizations. The main source of the data will be the i94 immigration data supplied from the U.S. National Tourism and Trade Office. This data contains information about a visit to the United States by a foreign citizen. To supplement this immigration data, two additional data sources will be used, U.S. City demographic data from OpenSoft, Airport Codes Table, and i94 country codes. These data sets will allow the end user to investigate immigration patterns and traffic based on airport usage and demographics of visited U.S. states. \n",
    "\n",
    "The tools used in this project include Python, Spark, and Amazon S3. Both of the tools (Python and Spark) are incredibly useful in dealing with large datasets and performing ETL (extract, transform, and load) tasks such as this project. Amazon S3 is a reliable cloud-based storage system used for housing big data which makes it ideal for this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = 'immigration_data_sample.csv'\n",
    "df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1'),\n",
       " Row(cicid=5748518.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='NV', depdate=20591.0, i94bir=32.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1984.0, dtaddto='10292016', gender='F', insnum=None, airline='VA', admnum=94955622830.0, fltno='00007', visatype='B1'),\n",
       " Row(cicid=5748519.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20582.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956406530.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748520.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='F', insnum=None, airline='DL', admnum=94956451430.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748521.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=28.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956388130.0, fltno='00040', visatype='B1')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|cicid|\n",
      "+-----+\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|i94yr|\n",
      "+-----+\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n",
      "+------+\n",
      "|i94mon|\n",
      "+------+\n",
      "|   0.0|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|i94cit|\n",
      "+------+\n",
      "|   0.0|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|i94res|\n",
      "+------+\n",
      "|   0.0|\n",
      "+------+\n",
      "\n",
      "+-------+\n",
      "|i94port|\n",
      "+-------+\n",
      "|    0.0|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|arrdate|\n",
      "+-------+\n",
      "|    0.0|\n",
      "+-------+\n",
      "\n",
      "+--------------------+\n",
      "|             i94mode|\n",
      "+--------------------+\n",
      "|0.007718857880324115|\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+\n",
      "|          i94addr|\n",
      "+-----------------+\n",
      "|4.928183940060324|\n",
      "+-----------------+\n",
      "\n",
      "+---------------+\n",
      "|        depdate|\n",
      "+---------------+\n",
      "|4.6008591508675|\n",
      "+---------------+\n",
      "\n",
      "+--------------------+\n",
      "|              i94bir|\n",
      "+--------------------+\n",
      "|0.025901774142342845|\n",
      "+--------------------+\n",
      "\n",
      "+-------+\n",
      "|i94visa|\n",
      "+-------+\n",
      "|    0.0|\n",
      "+-------+\n",
      "\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n",
      "+--------------------+\n",
      "|            dtadfile|\n",
      "+--------------------+\n",
      "|3.229647648671178E-5|\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+\n",
      "|         visapost|\n",
      "+-----------------+\n",
      "|60.75774639062653|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|            occup|\n",
      "+-----------------+\n",
      "|99.73755883206898|\n",
      "+-----------------+\n",
      "\n",
      "+--------------------+\n",
      "|             entdepa|\n",
      "+--------------------+\n",
      "|0.007686561403837...|\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+\n",
      "|          entdepd|\n",
      "+-----------------+\n",
      "|4.470768943579024|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|          entdepu|\n",
      "+-----------------+\n",
      "|99.98733978121722|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|          matflag|\n",
      "+-----------------+\n",
      "|4.470768943579024|\n",
      "+-----------------+\n",
      "\n",
      "+--------------------+\n",
      "|             biryear|\n",
      "+--------------------+\n",
      "|0.025901774142342845|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|             dtaddto|\n",
      "+--------------------+\n",
      "|0.015405419284161517|\n",
      "+--------------------+\n",
      "\n",
      "+------------------+\n",
      "|            gender|\n",
      "+------------------+\n",
      "|13.379429017673601|\n",
      "+------------------+\n",
      "\n",
      "+-----------------+\n",
      "|           insnum|\n",
      "+-----------------+\n",
      "|96.32763225164898|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|          airline|\n",
      "+-----------------+\n",
      "|2.700857439154246|\n",
      "+-----------------+\n",
      "\n",
      "+------+\n",
      "|admnum|\n",
      "+------+\n",
      "|   0.0|\n",
      "+------+\n",
      "\n",
      "+------------------+\n",
      "|             fltno|\n",
      "+------------------+\n",
      "|0.6313638188387285|\n",
      "+------------------+\n",
      "\n",
      "+--------+\n",
      "|visatype|\n",
      "+--------+\n",
      "|     0.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_rows = df_spark.count()\n",
    "for c in df_spark.columns:\n",
    "    df_spark.select([(100*count(when(col(c).isNull() | isnan(col(c)), c))/num_rows).alias(c)]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Handling missing values\n",
    "Since the main purpose of this data set is to allow the end user to analyze immigration logistic and demographic patterns I had to make decision regarding the missing values discovered above. One of the supporting data sets will include US state demographics. Since there are only 5% of the rows missing i94 addresses which is coded as the US state visited then I am going to drop all rows missing that field. I have also decided to include visatype in my tables going forward since the data is complete and it could be used to analyze visits by visa type which could potentially lead to insights and logistics planning. It is disappointing that 13% of the gender field is missing but I am still going to include this field in the final table. The end user will just need to exercise caution when using this field knowing that it may be missing for a decently large portion of the rows. I will also be dropping any rows that are missing the \"i94bir\" column which represents the age of the traveler. I want the end user to have access to demographics related to age groups when conducting analysis. Finally, it is worth noting that the depature date is missing for around 5% of the data. I am going to leave the length of stay missing for rows without a depdate. I do not have enough information to make an assumption. I would rather the end user known there is missing data than make a false assumption that leads them astray. For the other data sets, I will be making the i94 country code via Excel so it will not contain missing values. Also the airport codes file will have a lot of missing values for the iata_code. However, since this code is necessary as a tie to my facts table, I will be dropping all rows that do not have an iata_code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|i94addr|count(i94addr)|\n",
      "+-------+--------------+\n",
      "|     FL|        621701|\n",
      "|     NY|        553677|\n",
      "|     CA|        470386|\n",
      "|     HI|        168764|\n",
      "|     TX|        134321|\n",
      "|     NV|        114609|\n",
      "|     GU|         94107|\n",
      "|     IL|         82126|\n",
      "|     NJ|         76531|\n",
      "|     MA|         70486|\n",
      "|     WA|         55792|\n",
      "|     GA|         44663|\n",
      "|     MI|         32101|\n",
      "|     VA|         31399|\n",
      "|     PA|         30293|\n",
      "|     DC|         28228|\n",
      "|     NE|         26574|\n",
      "|     MD|         25360|\n",
      "|     NC|         23375|\n",
      "|     LA|         22655|\n",
      "|     AZ|         20218|\n",
      "|     OH|         18089|\n",
      "|     CO|         15874|\n",
      "|     CT|         13991|\n",
      "|     OR|         12574|\n",
      "+-------+--------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### look at data in i94addr\n",
    "df_spark.select('i94addr').groupBy('i94addr').agg({'i94addr':'count'}).sort(desc('count(i94addr)')).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|i94addr|count(i94addr)|\n",
      "+-------+--------------+\n",
      "|   null|             0|\n",
      "|     71|             1|\n",
      "|     52|             1|\n",
      "|     HR|             1|\n",
      "|     73|             1|\n",
      "|     RU|             1|\n",
      "|     XN|             1|\n",
      "|     CG|             1|\n",
      "|     RF|             1|\n",
      "|     UR|             1|\n",
      "|     UL|             1|\n",
      "|     ZN|             1|\n",
      "|     EV|             1|\n",
      "|     KF|             1|\n",
      "|     YH|             1|\n",
      "|     S6|             1|\n",
      "|     RO|             1|\n",
      "|     VL|             1|\n",
      "|     RA|             1|\n",
      "|     EX|             1|\n",
      "|     JC|             1|\n",
      "|     PD|             1|\n",
      "|     FC|             1|\n",
      "|     TC|             1|\n",
      "|     85|             1|\n",
      "+-------+--------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### look at data in i94addr\n",
    "df_spark.select('i94addr').groupBy('i94addr').agg({'i94addr':'count'}).sort('count(i94addr)').show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Handling invalid codes\n",
    "This project is solely to focus on visits to U.S. states. There appear to be values that do not match up to any U.S. states. Therefore, I will be using a list of state codes to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc of data remaining: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90.8609691591257"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check to see how much data will be lost\n",
    "states_list = pd.read_csv(\"US_state_list.csv\")\n",
    "state_codes = list(states_list.Code)\n",
    "\n",
    "num_rows = df_spark.count()\n",
    "print(\"Perc of data remaining: \")\n",
    "100 * df_spark.where(col(\"i94addr\").isin(state_codes)).count() / num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|i94port|count(i94port)|\n",
      "+-------+--------------+\n",
      "|    NYC|        485916|\n",
      "|    MIA|        343941|\n",
      "|    LOS|        310163|\n",
      "|    SFR|        152586|\n",
      "|    ORL|        149195|\n",
      "|    HHW|        142720|\n",
      "|    NEW|        136122|\n",
      "|    CHI|        130564|\n",
      "|    HOU|        101481|\n",
      "|    FTL|         95977|\n",
      "|    ATL|         92579|\n",
      "|    LVG|         89280|\n",
      "|    AGA|         80919|\n",
      "|    WAS|         74835|\n",
      "|    DAL|         71809|\n",
      "|    BOS|         57354|\n",
      "|    SEA|         47719|\n",
      "|    PHO|         38890|\n",
      "|    DET|         37832|\n",
      "|    TAM|         25632|\n",
      "|    PHI|         24973|\n",
      "|    DUB|         24371|\n",
      "|    SAI|         23628|\n",
      "|    TOR|         20886|\n",
      "|    DEN|         18260|\n",
      "+-------+--------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### look at data in i94port\n",
    "df_spark.select('i94port').groupBy('i94port').agg({'i94port':'count'}).sort(desc('count(i94port)')).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|i94port|count(i94port)|\n",
      "+-------+--------------+\n",
      "|    COO|             1|\n",
      "|    PHF|             1|\n",
      "|    HNN|             1|\n",
      "|    NC8|             1|\n",
      "|    PCF|             1|\n",
      "|    CNC|             1|\n",
      "|    VNB|             1|\n",
      "|    RIO|             1|\n",
      "|    CPX|             1|\n",
      "|    REN|             1|\n",
      "|    YIP|             1|\n",
      "|    BWM|             1|\n",
      "|    LWT|             1|\n",
      "|    ANA|             1|\n",
      "|    SCH|             1|\n",
      "|    MAI|             1|\n",
      "|    ERC|             1|\n",
      "|    NIG|             1|\n",
      "|    MND|             1|\n",
      "|    NEC|             2|\n",
      "|    CRP|             2|\n",
      "|    DVL|             2|\n",
      "|    RYY|             2|\n",
      "|    PSM|             2|\n",
      "|    MTH|             2|\n",
      "+-------+--------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### look at data in i94port\n",
    "df_spark.select('i94port').groupBy('i94port').agg({'i94port':'count'}).sort('count(i94port)').show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There does not appear to be anything crazy with the i94port codes. They all are 3 digit codes. It is possible that they will not have a matching iata_code in the airports table but initial analysis looks fine. There is one value \"XXX\" which is the code for unknown or unreported values. Checking below to see how often that happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data coded as XXX which means unknown: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11374819018619887"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Percentage of data coded as XXX which means unknown: \")\n",
    "100 * df_spark.select('i94port').filter(df_spark.i94port == 'XXX').count() / num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Low percentage of rows so will be filtering this data out in the final table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Model and Data Dictionary\n",
    "\n",
    "A storage method known as a STAR schema will be applied to make the data end user friendly. The idea behind this schema is a single fact table surrounded by supporting dimensional tables. This schema requires more storage than a tradition OLTP relational database but comes with the benefit of easier usage by analysts. Since storage on S3 is relatively cheap and extremely reliable, the space tradeoff is an easy decision to make. The final deliverable will be a Python ETL script that can be run on demand to create the analytic tables which will be saved as Parquet files (file type that plays well with Spark) in an S3 bucket. See below for the schema\n",
    "\n",
    "The i94 data includes a variety of information about each visit to the United States from a foreign citizen. The data extracted in this project will include the following:\n",
    "- immigration_record_id\n",
    "- country_of_citizenship_i94_code = country of citizenship for traveler\n",
    "- country_of_residence_i94_code = country of residence for traveler\n",
    "- state_visited_code = U.S. state visited\n",
    "- airport_iata_code = airport used\n",
    "- airline_used \n",
    "- visatype\n",
    "- month_of_arrival = month of arrival in U.S.\n",
    "- year_of_arrival = year of arrival in U.S.\n",
    "- traveler_age\n",
    "- traveler_gender\n",
    "- visit_length_days\n",
    "\n",
    "The supplemental data set, U.S. city demographics contains information regarding populations for U.S. cities with each row representing a ethnic group in that city. For this project, the data will only be extracted a state level with no regards to ethnic groups. The following data will be used from each state:\n",
    "- state_code = 2 digit state code\n",
    "- name = name of state\n",
    "- total_male_population\n",
    "- total_female_population\n",
    "- total_foreign_born_residents\n",
    "\n",
    "The Airport Code Table includes a lot of data about airports in the world, including information such as country, region, and type. The data will be limited to only airports located in the United States and will include:\n",
    "- iata_code = code used to uniquely identify airport\n",
    "- name = name of airport\n",
    "- type = type of airport (small, heliport, etc)\n",
    "- state = state where airport is located\n",
    "- municipality = municipality in which airport is located\n",
    "- longitude\n",
    "- latitude\n",
    "\n",
    "There will also be a small file containing i94 country codes that will be used to determine the country of residence and citizenship of the travelers. The table will contain the following:\n",
    "- i94 code\n",
    "- name of location\n",
    "\n",
    "### Schema\n",
    "![immigration_schema](immigration_schema_erd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Pipelines to Model the Data\n",
    "\n",
    "#### Read in AWS Credentials to allow access to output final Parquet files into S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dl.cfg']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read in and clean i94 Immigration Data\n",
    "\n",
    "This data is stored in SaS format. You will need to store the link to the i94 data in a file named dl.cfg under the variable i94_DATA. This data can be stored in S3 or local storage depending on the number of files you are interested in ingesting. Since this data is in SaS format, the ingestion step will require use of PySparks read_format function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_file_path = config.get('DATA', 'i94_DATA')\n",
    "immigration_data_full = spark.read.format('com.github.saurfang.sas.spark').load(i94_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Select only the columns used in the schema described above. Columns have different names in the i94 data so one part of the pipeline will be renaming to match the schema.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_data = immigration_data_full.select(col('cicid').alias('immigration_record_id'),\n",
    "                                            col('i94cit').alias('country_of_citizenship_i94_code'),\n",
    "                                            col('i94res').alias('country_of_residence_i94_code'),\n",
    "                                            col('i94addr').alias('state_visited_code'),\n",
    "                                            col('i94port').alias('airport_iata_code'),\n",
    "                                            col('airline').alias('airline_used'),\n",
    "                                            col('visatype'),\n",
    "                                            col('i94mon').alias('month_of_arrival'),\n",
    "                                            col('i94yr').alias('year_of_arrival'),\n",
    "                                            col('i94bir').alias('traveler_age'),\n",
    "                                            col('gender').alias('traveler_gender'),\n",
    "                                            col('arrdate'),\n",
    "                                            col('depdate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### read in states list to filter state_visited down to only US states\n",
    "states_list = pd.read_csv('US_state_list.csv')\n",
    "state_codes = list(states_list.Code)\n",
    "\n",
    "immigration_data = immigration_data.where(col('i94addr').isin(state_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### filter out any data with an airport_iata_code = \"XXX\"\n",
    "immigration_data = immigration_data.filter(immigration_data.airport_iata_code != 'XXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### drop missing values in traveler age\n",
    "immigration_data = immigration_data.where(col('traveler_age').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(immigration_record_id=7.0, country_of_citizenship_i94_code=254.0, country_of_residence_i94_code=276.0, state_visited_code='AL', airport_iata_code='ATL', airline_used=None, visatype='F1', month_of_arrival=4.0, year_of_arrival=2016.0, traveler_age=25.0, traveler_gender='M', visit_length_days=None),\n",
       " Row(immigration_record_id=15.0, country_of_citizenship_i94_code=101.0, country_of_residence_i94_code=101.0, state_visited_code='MI', airport_iata_code='WAS', airline_used='OS', visatype='B2', month_of_arrival=4.0, year_of_arrival=2016.0, traveler_age=55.0, traveler_gender='M', visit_length_days=146.0),\n",
       " Row(immigration_record_id=16.0, country_of_citizenship_i94_code=101.0, country_of_residence_i94_code=101.0, state_visited_code='MA', airport_iata_code='NYC', airline_used='AA', visatype='B2', month_of_arrival=4.0, year_of_arrival=2016.0, traveler_age=28.0, traveler_gender=None, visit_length_days=22.0),\n",
       " Row(immigration_record_id=17.0, country_of_citizenship_i94_code=101.0, country_of_residence_i94_code=101.0, state_visited_code='MA', airport_iata_code='NYC', airline_used='AA', visatype='B2', month_of_arrival=4.0, year_of_arrival=2016.0, traveler_age=4.0, traveler_gender=None, visit_length_days=22.0),\n",
       " Row(immigration_record_id=18.0, country_of_citizenship_i94_code=101.0, country_of_residence_i94_code=101.0, state_visited_code='MI', airport_iata_code='NYC', airline_used='AZ', visatype='B1', month_of_arrival=4.0, year_of_arrival=2016.0, traveler_age=57.0, traveler_gender=None, visit_length_days=10.0)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### calculate length of stay\n",
    "immigration_data = immigration_data.withColumn('visit_length_days', col('depdate')-col('arrdate'))\n",
    "### drop arrdate and depdate columns\n",
    "immigration_data = immigration_data.drop('arrdate','depdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### write immigration data to parquet file, partition by year and month\n",
    "immigration_data.write.partitionBy('year_of_arrival','month_of_arrival').mode('overwrite').parquet('immigration_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read in and clean airport codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airports_input_file_path = config.get('DATA', 'AIRPORT_INPUT_DATA')\n",
    "airports = pd.read_csv('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### keep only rows with 3 digit iata codes and in the US\n",
    "three_digit_code = lambda x: True if not pd.isna(x) and len(x) == 3 else False\n",
    "valid_codes = airports.iata_code.apply(three_digit_code)\n",
    "airports_clean = airports[valid_codes]\n",
    "airports_clean = airports_clean[airports_clean.iso_country == 'US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### extract columns\n",
    "columns_to_select = ['type','name','iso_region','municipality','iata_code','coordinates']\n",
    "airports_clean = airports_clean[columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### create new \"state\" column by extracting code from iso_region\n",
    "extract_state_from_region = lambda x: x.split('-')[1]\n",
    "airports_clean['state'] = airports_clean.iso_region.apply(extract_state_from_region)\n",
    "### drop iso_region\n",
    "airports_clean = airports_clean.drop('iso_region',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### create latitude and longitude columns from coordinates\n",
    "extract_lat_from_coords = lambda x: x.split(',')[0]\n",
    "extract_long_from_coords = lambda x: x.split(',')[1]\n",
    "airports_clean['latitude'] = airports_clean.coordinates.apply(extract_lat_from_coords)\n",
    "airports_clean['longitude'] = airports_clean.coordinates.apply(extract_long_from_coords)\n",
    "### drop coordinates data\n",
    "airports_clean = airports_clean.drop('coordinates',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### save clean file\n",
    "airport_output_file_path = config.get('DATA', 'AIRPORT_OUTPUT_DATA')\n",
    "airports_clean.to_csv(airport_output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read in and aggregate US demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_demo_input_file_path = config.get('DATA', 'US_DEMO_INPUT_DATA')\n",
    "state_demo = pd.read_csv(state_demo_input_file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### extract only necessary columns\n",
    "columns_to_select = ['City','State','Male Population','Female Population','Foreign-born','State Code']\n",
    "state_demo = state_demo[columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Foreign-born</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>Texas</td>\n",
       "      <td>TX</td>\n",
       "      <td>65212.0</td>\n",
       "      <td>60664.0</td>\n",
       "      <td>8129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Akron</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>OH</td>\n",
       "      <td>96886.0</td>\n",
       "      <td>100667.0</td>\n",
       "      <td>10024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alafaya</td>\n",
       "      <td>Florida</td>\n",
       "      <td>FL</td>\n",
       "      <td>39504.0</td>\n",
       "      <td>45760.0</td>\n",
       "      <td>15842.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alameda</td>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "      <td>37747.0</td>\n",
       "      <td>40867.0</td>\n",
       "      <td>18841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>GA</td>\n",
       "      <td>31695.0</td>\n",
       "      <td>39414.0</td>\n",
       "      <td>861.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      City       State State Code  Male Population  Female Population  \\\n",
       "0  Abilene       Texas         TX          65212.0            60664.0   \n",
       "1    Akron        Ohio         OH          96886.0           100667.0   \n",
       "2  Alafaya     Florida         FL          39504.0            45760.0   \n",
       "3  Alameda  California         CA          37747.0            40867.0   \n",
       "4   Albany     Georgia         GA          31695.0            39414.0   \n",
       "\n",
       "   Foreign-born  \n",
       "0        8129.0  \n",
       "1       10024.0  \n",
       "2       15842.0  \n",
       "3       18841.0  \n",
       "4         861.0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### aggregate by city by state by taking minimum of population numbers since the rows are \n",
    "### broken out by ethnic group but the population numbers are for the whole city\n",
    "### which causes them to get replicated\n",
    "state_demo_grouped = state_demo.groupby(['City','State','State Code']).agg({'Male Population':'min',\n",
    "                                                                            'Female Population':'min',\n",
    "                                                                            'Foreign-born':'min'})\n",
    "state_demo_grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### now aggregate by state since that is the level at which we are interested\n",
    "state_demo_grouped = state_demo_grouped.groupby(['State','State Code']).agg({'Male Population':'sum',\n",
    "                                                                            'Female Population':'sum',\n",
    "                                                                            'Foreign-born':'sum'})\n",
    "state_demo_grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### rename columns to match schema\n",
    "state_demo_final = state_demo_grouped.rename({'State':'name',\n",
    "                                             'State Code':'state_code',\n",
    "                                             'Male Population':'total_male_population',\n",
    "                                             'Female Population':'total_female_population',\n",
    "                                             'Foreign-born':'total_foreign_born_residents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### export data\n",
    "state_demo_output_file_path = config.get('DATA', 'US_DEMO_OUTPUT_DATA')\n",
    "state_demo_final.to_csv(state_demo_output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Quality Checks\n",
    "\n",
    "There are two main data quality check I will be performing. The first and most obvious one is to make sure that there is actually data in the analytic table files. The second check will be to ensure that certain columns do not contain any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Missing Value Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Immigration data has 2811325 records!\n"
     ]
    }
   ],
   "source": [
    "### check immigrations data to make sure that data has been loaded\n",
    "immigration_data_file_path = config.get('DATA', 'IMMIGRATION_OUTPUT_DATA')\n",
    "immigration_data = spark.read.parquet(immigration_data_file_path)\n",
    "\n",
    "num_records = immigration_data.count()\n",
    "if num_records == 0:\n",
    "    raise ValueError(\"Something went wrong! Immigration data has zero records.\")\n",
    "else:\n",
    "    print(\"Success! Immigration data has\", num_records, \"records!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Airport codes data has 2019 records!\n"
     ]
    }
   ],
   "source": [
    "### check airport codes\n",
    "airport_output_file_path = config.get('DATA', 'AIRPORT_OUTPUT_DATA')\n",
    "airport_codes = pd.read_csv(airport_output_file_path)\n",
    "\n",
    "num_records = airport_codes.shape[0]\n",
    "if num_records == 0:\n",
    "    raise ValueError(\"Something went wrong! Airport codes data has zero records.\")\n",
    "else:\n",
    "    print(\"Success! Airport codes data has\", num_records, \"records!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! US demographics data has 49 records!\n"
     ]
    }
   ],
   "source": [
    "### check US demographics data\n",
    "state_demo_output_file_path = config.get('DATA', 'US_DEMO_OUTPUT_DATA')\n",
    "state_demo = pd.read_csv(state_demo_output_file_path)\n",
    "\n",
    "num_records = state_demo.shape[0]\n",
    "if num_records == 0:\n",
    "    raise ValueError(\"Something went wrong! US demographics data has zero records.\")\n",
    "else:\n",
    "    print(\"Success! US demographics data has\", num_records, \"records!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Null Value Checks\n",
    "\n",
    "For the immigration data there are a few columns in which we do not want to have missing values: immigration_record_id, country_of_citizenship_i94_code, country_of_residence_i94_code,\n",
    "state_visited_code, traveler_age, airport_iata_code.\n",
    "\n",
    "For the airport and US demographics data, there should be no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! immigration_record_id has 0 missing values!\n",
      "Success! country_of_citizenship_i94_code has 0 missing values!\n",
      "Success! country_of_residence_i94_code has 0 missing values!\n",
      "Success! state_visited_code has 0 missing values!\n",
      "Success! traveler_age has 0 missing values!\n",
      "Success! airport_iata_code has 0 missing values!\n"
     ]
    }
   ],
   "source": [
    "### check immigrations data for missing values\n",
    "columns_to_check = ['immigration_record_id','country_of_citizenship_i94_code',\n",
    "                    'country_of_residence_i94_code','state_visited_code',\n",
    "                    'traveler_age','airport_iata_code']\n",
    "for column in columns_to_check:\n",
    "    num_missing_values = immigration_data.select([(count(when(col(column).isNull() | isnan(col(column)), c)))]).collect()[0][0]\n",
    "    if num_missing_values > 0:\n",
    "        raise ValueError('Something went wrong!', column, 'has', num_missing_values, 'missing_values')\n",
    "    else:\n",
    "        print('Success!', column, 'has 0 missing values!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The tools used in this project include Python, Spark, and Amazon S3. Both of the tools (Python and Spark) are incredibly useful in dealing with large datasets and performing ETL (extract, transform, and load) tasks such as this project. Amazon S3 is a reliable cloud-based storage system used for housing big data which makes it ideal for this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data update rate for this data would heavily depend on the analytics use case. However, since the files are based on month then it makes the most sense to update this data at the end of every month. The immigration data requires monetary investment so that may play a large role in how often the end user would want to update the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Future considerations\n",
    " * The data was increased by 100x.\n",
    " \n",
    "     If the data was to be increased by 100x then alterations would definitely need to be made. Rather than reading and writing the parquet files from local storage, a cloud based storage solution such as Amazon S3 would be absolutely necessary unless the end user already had access to capable on premise infrastructure. It would also essential to offload the data processing work to a cloud based or distrubed work environment. Right now Spark is being used locally but distributing the work load would have to happen with that magnitude of increase. Amazon also offers solutions such as EMR and EC2 to handle this type of workload.\n",
    "    \n",
    "    \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    "     Since this is monthly data, I wouldn't see the business case for a daily updating dashboard. However, should there be the need, you would need a pipeline to extract the data from the relevant sources. Rather than using static csv and sas files that have been downloaded manually, you would need to find a way to automate the data retrieval. This could be in the form of API endpoints, FTP sites, web scraping scripts, etc. In order to properly orchestrate all of these automated processes, it would be good to use a tool such as Airflow, Luigi, or any other DAG pipeline scheduler. This would allow monitoring of task statuses and task dependencies such as making sure the data retrieval automation had been completed before the processing steps. Also it would allow the tasks to retry and notify you on task failure.\n",
    "     \n",
    "     \n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    "     Depending on the budget, hosting the data on the cloud would be the best solution for scaling up to hundreds of users. By storing the data in a S3 bucket or similar cloud service. Access to the data can then be given based on IAM roles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
